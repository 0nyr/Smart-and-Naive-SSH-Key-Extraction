{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testzone\n",
    "Script tester zone. Used to try code and ideas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test a Scikit-learn Classifier\n",
    "In this example, the `make_classification` function is used to generate some synthetic data for the example, but in a real-world scenario, you would use your own data. The data is first split into training and test sets using the `train_test_split` function. The `RandomForestClassifier` is initialized with a random state. The classifier is then fit to the training data using the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of feature vectors X:\n",
      "[[ 1.27815198 -0.41644753  0.89181112  0.77129444]\n",
      " [ 1.35681817 -1.51465569  1.82132242  0.42081175]\n",
      " [ 1.53341056  2.06290707 -1.01967188  1.87609016]\n",
      " [ 0.42064934  0.05455201  0.13725671  0.32493018]\n",
      " [-0.88825673 -1.10088618  0.51393811 -1.05185003]]\n",
      "and associated labels y:\n",
      "[1 1 1 1 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple example of how to train a model and test it.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate some synthetic data for the example\n",
    "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=0)\n",
    "\n",
    "# display the first 5 rows of the data\n",
    "print(\"Sample of feature vectors X:\", X[:5], \"and associated labels y:\", y[:5], sep=\"\\n\")\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# initialize the classifier[:10]\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method is then used to make predictions on the test data, and the `accuracy_score` function is used to calculate the accuracy of the model. The `classification_report` function is also used to generate a more detailed report of the model's performance, including precision, recall, and F1-score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of predicted labels:\n",
      "[1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0]\n",
      "versus actual labels:\n",
      "[1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0]\n",
      "Accuracy: 97.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97        95\n",
      "           1       0.96      0.98      0.97       105\n",
      "\n",
      "    accuracy                           0.97       200\n",
      "   macro avg       0.97      0.97      0.97       200\n",
      "weighted avg       0.97      0.97      0.97       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\n",
    "    \"Sample of predicted labels:\", y_pred[:20], \n",
    "    \"versus actual labels:\", y_test[:20], sep=\"\\n\"\n",
    ")\n",
    "\n",
    "# calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `confusion_matrix` function is used to calculate a confusion matrix, which is a 2x2 table that shows the number of true positives, false positives, true negatives, and false negatives. The `roc_curve` function is used to calculate the false positive rate and true positive rate, which are used to plot the ROC curve. The `auc` function is used to calculate the area under the ROC curve, which is a measure of the classifier's performance. Finally, the ROC curve is plotted using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[ 91   4]\n",
      " [  2 103]]\n",
      "True Positives:  103\n",
      "True Negatives:  91\n",
      "False Positives:  4\n",
      "False Negatives:  2\n",
      "AUC: 0.97\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAUC: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(roc_auc))\n\u001b[1;32m     18\u001b[0m \u001b[39m# plot the ROC curve\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m plt\u001b[39m.\u001b[39mplot(fpr, tpr, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mROC curve (area = \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(roc_auc))\n\u001b[1;32m     21\u001b[0m plt\u001b[39m.\u001b[39mplot([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mk--\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# random predictions curve\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", cm)\n",
    "print(\"True Positives: \", cm[1, 1])\n",
    "print(\"True Negatives: \", cm[0, 0])\n",
    "print(\"False Positives: \", cm[0, 1])\n",
    "print(\"False Negatives: \", cm[1, 0])\n",
    "\n",
    "# calculate the false positive rate and true positive rate\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "# calculate the area under the ROC curve\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC: {:.2f}\".format(roc_auc))\n",
    "\n",
    "# plot the ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a different classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# machine learning\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from classes import *\n",
    "import utils\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program paths are OK.\n"
     ]
    }
   ],
   "source": [
    "# program parameters\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ProgramParameters:\n",
    "    \"\"\"\n",
    "    Wrapper class for program parameters.\n",
    "    \"\"\"\n",
    "    MODEL_DIR_PATH = os.environ['HOME'] + \"/Documents/code/phdtrack/Smart-and-Naive-SSH-Key-Extraction/smart_ssh_key_extractor/new_models\"\n",
    "    TRAINING_DIR_PATH = os.environ['HOME'] + '/Documents/code/phdtrack/phdtrack_data/Training/Training/scp/'\n",
    "    VALIDATION_DIR_PATH = os.environ['HOME'] + '/Documents/code/phdtrack/phdtrack_data/Validation/Validation/scp/V_7_8_P1/16'\n",
    "    RESULTS_PATH = os.environ['HOME'] + \"/Documents/code/phdtrack/Smart-and-Naive-SSH-Key-Extraction/smart_ssh_key_extractor/results\"\n",
    "    \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        if (\n",
    "            utils.check_path_exists(self.MODEL_DIR_PATH) and\n",
    "            utils.check_path_exists(self.TRAINING_DIR_PATH) and\n",
    "            utils.check_path_exists(self.VALIDATION_DIR_PATH) and\n",
    "            utils.check_path_exists(self.RESULTS_PATH)\n",
    "        ):\n",
    "            print(\"Program paths are OK.\")\n",
    "        else:\n",
    "            print(\"Program paths are NOT OK.\")\n",
    "            exit(1)\n",
    "\n",
    "\n",
    "PARAMS = ProgramParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw files:  8501\n",
      "Number of keys json files:  8501\n",
      "Number of .raw files to load: 8501\n",
      "Number of block datas:  23290878\n",
      "Number of invalid blocks (ignored):  1737\n",
      "Number of features:  128\n",
      "Number of samples:  23290878\n",
      "Number of labels:  23290878\n"
     ]
    }
   ],
   "source": [
    "# get the data, create datasets\n",
    "file_paths, key_paths = utils.get_dataset_file_paths(PARAMS.TRAINING_DIR_PATH)\n",
    "print(\"Number of raw files: \", len(file_paths))\n",
    "print(\"Number of keys json files: \", len(key_paths))\n",
    "\n",
    "all_block_datas, nb_invalid_blocks = train_utils.create_dataset(PARAMS.TRAINING_DIR_PATH)\n",
    "print(\"Number of block datas: \", len(all_block_datas))\n",
    "print(\"Number of invalid blocks (ignored): \", nb_invalid_blocks)\n",
    "\n",
    "# count and remove the invalid block datas\n",
    "invalid_block_datas = 0\n",
    "for block_data in all_block_datas:\n",
    "    if block_data.label == -1:\n",
    "        invalid_block_datas += 1\n",
    "\n",
    "\n",
    "# transform the data into lists of samples and labels\n",
    "X = []\n",
    "y = []\n",
    "for block_data in all_block_datas:\n",
    "    sample = block_data.dataset\n",
    "    label = block_data.label\n",
    "\n",
    "    X.append(sample)\n",
    "    y.append(label)\n",
    "\n",
    "print(\"Number of features: \", len(X[0]))\n",
    "print(\"Number of samples: \", len(X))\n",
    "print(\"Number of labels: \", len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid block datas:  78297\n",
      "Number of empty block datas:  23212581\n"
     ]
    }
   ],
   "source": [
    "# print the number of valid and empty block datas\n",
    "valid_block_datas = 0\n",
    "empty_block_datas = 0\n",
    "\n",
    "for block_data in all_block_datas:\n",
    "    if block_data.label == BlockType.VALID.value:\n",
    "        valid_block_datas += 1\n",
    "    elif block_data.label == BlockType.EMPTY.value:\n",
    "        empty_block_datas += 1\n",
    "    else:\n",
    "        print(\"label: \", block_data.label)\n",
    "\n",
    "print(\"Number of valid block datas: \", valid_block_datas)\n",
    "print(\"Number of empty block datas: \", empty_block_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using undersampling\n",
      "Time taken for undersampling: 43.215010\n"
     ]
    }
   ],
   "source": [
    "# resample the data\n",
    "\n",
    "# Use SMOTE oversampling\n",
    "USE_SMOTE_OVERSAMPLING = False\n",
    "\n",
    "if USE_SMOTE_OVERSAMPLING:\n",
    "    print(\"Using SMOTE oversampling\")\n",
    "    start = time.time()\n",
    "    sm = SMOTE()\n",
    "    x_train, y_train = sm.fit_resample(X, y)\n",
    "    end = time.time()\n",
    "    print('Time taken for resampling: %f' % (end - start))\n",
    "\n",
    "# undersampling using RandomUnderSampler\n",
    "USE_UNDERSAMPLING = True\n",
    "\n",
    "if USE_UNDERSAMPLING:\n",
    "    print(\"Using undersampling\")\n",
    "    start = time.time()\n",
    "    rus = RandomUnderSampler()\n",
    "    x_train, y_train = rus.fit_resample(X, y)\n",
    "    end = time.time()\n",
    "    print('Time taken for undersampling: %f' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data <TRAINING>: 156594\n",
      "Number of training labels <TRAINING>: 156594\n",
      "Number of valid block datas <TRAINING>: 78297\n",
      "Number of empty block datas <TRAINING>: 78297\n"
     ]
    }
   ],
   "source": [
    "# print info about the training data\n",
    "print(\"Number of training data <TRAINING>:\", len(x_train))\n",
    "print(\"Number of training labels <TRAINING>:\", len(y_train))\n",
    "\n",
    "# print the number of valid and empty block datas\n",
    "valid_block_datas = 0\n",
    "empty_block_datas = 0\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == BlockType.VALID.value:\n",
    "        valid_block_datas += 1\n",
    "    elif y_train[i] == BlockType.EMPTY.value:\n",
    "        empty_block_datas += 1\n",
    "    else:\n",
    "        print(\"Big problem here: \", y_train[i])\n",
    "\n",
    "print(\"Number of valid block datas <TRAINING>:\", valid_block_datas)\n",
    "print(\"Number of empty block datas <TRAINING>:\", empty_block_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 0.1, 'class_weight': {0: 1, 1: 100}, 'kernel': 'rbf'}\n",
      "Time taken for training the classifier on resampled data: 16110.192132\n"
     ]
    }
   ],
   "source": [
    "# train a new classifier on the resampled data\n",
    "RANDOM_FOREST = False\n",
    "BAGGING = False\n",
    "SVC_CLF = True\n",
    "\n",
    "if BAGGING:\n",
    "    start = time.time()\n",
    "    custom_clf = BaggingClassifier(estimator=RandomForestClassifier(n_estimators=5), n_estimators=3, n_jobs=8, random_state=0)\n",
    "    custom_clf.fit(X=np.array(x_train), y=y_train)\n",
    "    end = time.time()\n",
    "    print('Time taken for training the classifier on resampled data: %f' % (end - start))\n",
    "elif RANDOM_FOREST:\n",
    "    start = time.time()\n",
    "    custom_clf = RandomForestClassifier(n_estimators=5, n_jobs=8, random_state=0)\n",
    "    custom_clf.fit(X=np.array(x_train), y=y_train)\n",
    "    end = time.time()\n",
    "    print('Time taken for training the classifier on resampled data: %f' % (end - start))\n",
    "elif SVC_CLF:\n",
    "    start = time.time()\n",
    "\n",
    "    # set the parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10], \n",
    "        'kernel': ['rbf'],\n",
    "        'class_weight': {0: 1, 1: 100}\n",
    "    }\n",
    "\n",
    "    # initialize the classifier\n",
    "    svc = SVC()\n",
    "\n",
    "    # initialize the grid search #TODO: RandomizedSearchCV\n",
    "    grid_search = GridSearchCV(svc, param_grid, cv=5, scoring='recall', n_jobs=8)\n",
    "\n",
    "    # fit the grid search to the data\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # print the best parameters\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "    #get the best model\n",
    "    custom_clf = grid_search.best_estimator_\n",
    "\n",
    "    end = time.time()\n",
    "    print('Time taken for training the classifier on resampled data: %f' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classifier\n",
    "model_file_path = os.path.join(PARAMS.MODEL_DIR_PATH, \"new_classifier_svc_1.pkl\")\n",
    "with open(model_file_path, 'wb') as fp:\n",
    "    pickle.dump(custom_clf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifier\n",
    "model_file_path = os.path.join(PARAMS.MODEL_DIR_PATH, \"new_classifier_svc_1.pkl\")\n",
    "with open(model_file_path, 'rb') as fp:\n",
    "    custom_clf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test raw files:  181\n",
      "Number of test keys json files:  181\n",
      "Testing Dataset\n",
      "['ode', 'phdtrack', 'phdtrack_data', 'Validation', 'Validation', 'scp', 'V_7_8_P1', '16']\n",
      "Time taken for reading and testing: 1224.978734\n",
      "METRICS OF TEST SET\n",
      "2023-01-16 01:35:52.298346:\tAccuracy: 0.596368\n",
      "2023-01-16 01:35:52.298399:\tPrecision: 0.016778\n",
      "2023-01-16 01:35:52.298406:\tRecall: 1.000000\n",
      "2023-01-16 01:35:52.298411:\tF1-Measure: 0.033001\n",
      "2023-01-16 01:35:52.298499:\t\n",
      "Confusion Matrix:\n",
      "[[226976 155416]\n",
      " [     0   2652]]\n",
      "2023-01-16 01:35:52.298508:\tTrue Positives: 2652\n",
      "2023-01-16 01:35:52.298514:\tTrue Negatives: 226976\n",
      "2023-01-16 01:35:52.298520:\tFalse Positives: 155416\n",
      "2023-01-16 01:35:52.298525:\tFalse Negatives: 0\n",
      "Time taken for computing metrics: 1.069762\n"
     ]
    }
   ],
   "source": [
    "# testing model on the test data\n",
    "test_file_paths, test_key_paths = utils.get_dataset_file_paths(PARAMS.VALIDATION_DIR_PATH)\n",
    "print(\"Number of test raw files: \", len(test_file_paths))\n",
    "print(\"Number of test keys json files: \", len(test_key_paths))\n",
    "\n",
    "print('Testing Dataset')\n",
    "start = time.time()\n",
    "y_test, y_pred, df = utils.test(\n",
    "    clf=custom_clf, \n",
    "    file_paths=test_file_paths, \n",
    "    key_paths=test_key_paths\n",
    ")\n",
    "end = time.time()\n",
    "print('Time taken for reading and testing: %f' % (end - start))\n",
    "\n",
    "path = os.path.join(PARAMS.RESULTS_PATH, \"test_results_\" + str(datetime.now()) + \".csv\")\n",
    "df.to_csv(path)\n",
    "\n",
    "start = time.time()\n",
    "print('METRICS OF TEST SET')\n",
    "utils.print_metrics(y_test=y_test, y_pred=y_pred)\n",
    "end = time.time()\n",
    "print('Time taken for computing metrics: %f' % (end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phdtrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "828c4622dac9967a1e498ff679a89b1404201a56ae91cf46826d65c256cdc648"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
