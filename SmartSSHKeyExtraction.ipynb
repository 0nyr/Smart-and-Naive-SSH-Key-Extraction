{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbd18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from utils import print_, init, close, get_dataset_file_paths, load_models, read_key_files\n",
    "from timeit import default_timer as timer\n",
    "init() # Initialize the logging files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45500635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rle_representation(data):\n",
    "    reshaped = np.reshape(data, newshape=(int(len(data) / 8), 8))\n",
    "\n",
    "    # Here we take reshaped array and compute the numerical row and column wise gradient and count the number\n",
    "    # of zeroes in each row. If there are more than 4 zeros which means there is a pattern repeating and\n",
    "    # is not a key. This is a very conservative estimate for better recall\n",
    "    num_row = int(len(data) / 8)\n",
    "\n",
    "    # x_grad = np.abs(np.diff(reshaped.astype(int), axis=1, append=np.zeros((num_row, 1)))).astype(bool)\n",
    "    # y_grad = np.abs(np.diff(reshaped.astype(int), axis=0, append=np.zeros((1, 8)))).astype(bool)\n",
    "    # The above numerical gradient computation is transformed into a single step below\n",
    "\n",
    "    poss_key_locs = (np.count_nonzero(np.abs(np.diff(reshaped.astype(int), axis=1,\n",
    "                                                     append=np.zeros((num_row, 1)))).astype(bool) &\n",
    "                                      np.abs(np.diff(reshaped.astype(int), axis=0,\n",
    "                                                     append=np.zeros((1, 8)))).astype(bool),\n",
    "                                      axis=1) >= 4).astype(int)  # Changed from 4 to 3 to accommodate for 12 byte keys\n",
    "\n",
    "    # This part addresses the issue of 12 byte keys. There could be two identical characters next to each other in the\n",
    "    # last 4 bytes which would make it impossible for a key loc. We modify that if there is a possibility for a key\n",
    "    idx = 1\n",
    "    while idx < len(poss_key_locs):\n",
    "        # Last 4 characters must be zeros and first four should have at least 3 unique characters\n",
    "        if poss_key_locs[idx] == 0 and poss_key_locs[idx-1] == 1 and \\\n",
    "                all(reshaped[idx][4:]) == 0 and len(set(reshaped[idx][:4])) > 2:\n",
    "            poss_key_locs[idx] = 1\n",
    "        idx += 1\n",
    "\n",
    "    # Roll the data to the left\n",
    "    rolled = np.roll(poss_key_locs, -1)\n",
    "    # The key cannot start at the last byte and then the block contain the whole key.\n",
    "    # So the last value is set to False\n",
    "    rolled[-1] = False\n",
    "    poss_key_locs = (poss_key_locs & rolled).astype(int)\n",
    "\n",
    "    # Roll right and OR it. The whole operation is similar to the opening morphological operation\n",
    "    rolled = np.roll(poss_key_locs, 1)\n",
    "    rolled[0] = False\n",
    "\n",
    "    poss_key_locs = poss_key_locs | rolled\n",
    "\n",
    "    characters, counts = get_run_length_encoded(poss_key_locs)\n",
    "\n",
    "    cum_sum = [0]\n",
    "\n",
    "    for idx in range(len(counts)):\n",
    "        cum_sum.append(cum_sum[idx] + counts[idx])\n",
    "\n",
    "    cum_sum = [x * 8 for x in cum_sum]\n",
    "\n",
    "    # The last offset is not required for the cumulative sum\n",
    "    return characters, counts, cum_sum[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(data, offsets, keys, max_key_size=128):\n",
    "    data_blocks = []\n",
    "    labels = []\n",
    "    last_frame_added = False\n",
    "    key_count = [0] * len(keys)\n",
    "    for offset in offsets:\n",
    "        if offset + max_key_size > len(data):\n",
    "            curr_data = data[-max_key_size:]\n",
    "            last_frame_added = True\n",
    "        else:\n",
    "            curr_data = data[offset:offset+max_key_size]\n",
    "        data_blocks.append(curr_data)\n",
    "\n",
    "        found = [l_idx for l_idx in range(len(keys)) if keys[l_idx] in curr_data]\n",
    "\n",
    "        if len(found) > 0:\n",
    "            labels.append(1)\n",
    "            for key_idx in set(found):\n",
    "                key_count[key_idx] += 1\n",
    "\n",
    "        else:\n",
    "            labels.append(0)\n",
    "\n",
    "        if last_frame_added is True:\n",
    "            break\n",
    "\n",
    "    assert len(data_blocks) == len(labels)\n",
    "    assert sum(labels) > 0 and sum(labels) >= len(keys)\n",
    "    assert min(key_count) != 0\n",
    "    return data_blocks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ac242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoded_dataset(heap_paths, key_paths, max_key_size=128, deploy=False):\n",
    "\n",
    "    dataset = []\n",
    "    labels = []\n",
    "\n",
    "    for heap_path, key_path in zip(heap_paths, key_paths):\n",
    "\n",
    "        # Check if the key path corresponds to the heap path, then read the required data\n",
    "        if deploy is False:\n",
    "            assert (key_path[:-5] in heap_path)\n",
    "            curr_keys = read_key_files(key_path)\n",
    "            # Remove repeated keys. This is an issue for some older versions of OpenSSH\n",
    "            curr_keys = list(map(bytearray, set(tuple(x) for x in curr_keys)))\n",
    "\n",
    "        with open(heap_path, \"rb\") as fp:\n",
    "            data = bytearray(fp.read())\n",
    "\n",
    "        characters, counts, cum_sum = generate_rle_representation(data)\n",
    "\n",
    "        viable_offsets = [cum_sum[idx] for idx in range(len(cum_sum)) if characters[idx] == 1]\n",
    "        slices, curr_labels = get_slices(data=data, offsets=viable_offsets, max_key_size=max_key_size, keys=curr_keys)\n",
    "        dataset = dataset + slices\n",
    "        labels = labels + curr_labels\n",
    "\n",
    "    assert len(labels) == len(dataset)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b136d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_probable_slices(clf, heap_paths, key_paths, root_dir):\n",
    "    # Sort the heap paths and key paths, so it easier to group them by version and key length\n",
    "    heap_paths.sort()\n",
    "    key_paths.sort()\n",
    "    \n",
    "    # For each of the keys\n",
    "    for idx in range(len(key_paths)):\n",
    "        start = time.time()\n",
    "        dataset, curr_labels = build_encoded_dataset(heap_paths=[heap_paths[idx]],\n",
    "                                                     key_paths=[key_paths[idx]])\n",
    "\n",
    "        x_test = np.array(dataset).astype(int)\n",
    "        curr_pred = clf.predict(x_test)\n",
    "\n",
    "        path_idx = key_paths[idx].rfind(\"/\")\n",
    "        file_name = key_paths[idx][path_idx+1:-4] + \"txt\"\n",
    "        sub_dir = key_paths[idx][len(root_dir)+1:path_idx]\n",
    "        dir_path = os.path.join(WRITE_PATH, sub_dir)\n",
    "        path = os.path.join(dir_path, file_name)\n",
    "        if os.path.exists(dir_path) is False:\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "        with open(path, 'w') as fp:\n",
    "            for inner_idx, pred in enumerate(curr_pred):\n",
    "                if pred == 0:\n",
    "                    continue\n",
    "\n",
    "                temp = ''.join(format(x, '02x') for x in dataset[inner_idx])\n",
    "                fp.write(temp + \"\\n\")\n",
    "\n",
    "        end = time.time()\n",
    "        print_('Total time taken for file %s: %f' % (heap_paths[idx], (end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_length_encoded(data_block):\n",
    "\n",
    "    idx = 1\n",
    "    characters = []\n",
    "    counts = []\n",
    "    count = 1\n",
    "    curr_char = data_block[0]\n",
    "    while idx < len(data_block):\n",
    "        if data_block[idx] == curr_char:\n",
    "            idx += 1\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            characters.append(curr_char)\n",
    "            counts.append(count)\n",
    "\n",
    "            count = 1\n",
    "            curr_char = data_block[idx]\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    # Append the last character and count\n",
    "    characters.append(curr_char)\n",
    "    counts.append(count)\n",
    "\n",
    "    return bytearray(characters), counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = '/Users/stewartsentanoe/Documents/Uni Passau/projects/SmartVMI/Use Case 1 Data/Performance Test/V_7_1_P1/aes128-ctr'  # Path to the dataset\n",
    "WRITE_PATH = '/tmp/output/' # Output file path\n",
    "\n",
    "# Load the models\n",
    "clf = load_models(load_high_recall_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all the files with in the test directory\n",
    "# start = time.time()\n",
    "start = timer()\n",
    "heap_paths, key_paths = get_dataset_file_paths(TEST)\n",
    "end = timer()\n",
    "# end = time.time()\n",
    "print_('Time taken for finding all files: %f' % (end - start))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce85241",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_probable_slices(clf=clf, heap_paths=heap_paths, key_paths=key_paths, root_dir=TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "close() # Close the logging file pointers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
